{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence level language detection\n",
    "\n",
    "This code attempts to classify the language at Dáil debates text at sentence level. My aim is to separate out English text to create a cleaner and smaller dataset for analysis, but also to lay the groundwork for enabling searching debates in Irish.\n",
    "\n",
    "Members might make their contributions to Dáil debates are made in English or Irish, although the substantial majority use English. In some cases the entire contribution is mono-lingual but in other cases, a Member may alternate between Irish and English. Sometimmes a Member will even switch between Irish and English in the same sentence.\n",
    "\n",
    "As far as I'm aware, there were only three occasions where a contribution in a different language to Irish or English appears in the Official report: the programme for the First Dáil, which was in several languages; Francois Mitterand's address to the Dáil as French head of state on 26 February 1988, which was in French; and Helmut Kohl's address as German Chancellor on 2 October 1996, in German.\n",
    "\n",
    "I'm parsing language at the sentence level because of the propensity for Members to alternate between Irish and English in their speech. Parsing at the speech level would not give accurate results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools used\n",
    "\n",
    "I am trying out two alternative approaches. The first is a simple parser that counts the number of fadas (accented characters) and common English stopwords in a sentence. The second uses the [Polyglot](https://pypi.python.org/pypi/polyglot) library, which offers a language detection tool.\n",
    "\n",
    "The test data is stored in a Mongodb collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymongo, re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from polyglot.detect import Detector\n",
    "from polyglot.text import Text as Poly\n",
    "from datetime import datetime\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db = pymongo.MongoClient().texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic detector\n",
    "\n",
    "This script first attempts to identify sentence language simply by counting the number of fadas(\"áéíóú\") as a proportion of the total number of characters. \n",
    "\n",
    "Sentences with a score of greater than 0.01 are categorised as Irish (\"ga\"), and sentences with a score of less than 0.008 are classified as English (\"en\"). I set these thresholds after examining the results at different thresholds.\n",
    "\n",
    "Scores of between 0.008 and 0.01 were more difficult to differentiate, so I added a new step adapted from this post, which counds the intersection between the set of NLTK stopwords and the set of tokens in the sentence, as a proportion of the total number of tokens in the sentence. Scores of less than 0.1 are classified as Irish, and above 0.2 as English. This still leaves unclassified sentences with a score from 0.1 and 0.2 and to classify these I used the Polyglot library, as explained below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stops = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def detect_language(sent):\n",
    "    assert sent is not None\n",
    "    score = len(re.sub(\"[^áéíóú]\", \"\", sent.lower().string))/len(sent.string)\n",
    "    if score < 0.008:\n",
    "        return(score, \"English\")\n",
    "    elif 0.008 < score < 0.01:\n",
    "        tok_set = set(sent.tokens)\n",
    "        intersect = len(tok_set.intersection(stops))/len(tok_set)\n",
    "        if intersect < 0.1:\n",
    "            return(score, \"Irish\")\n",
    "        if 0.1 < intersect < 0.2:\n",
    "            lang = Detector(sent.string).language.name\n",
    "            return (score, lang)\n",
    "        else:\n",
    "            return (score, \"English\")\n",
    "    else:\n",
    "        return(score, \"Irish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polyglot detector\n",
    "\n",
    "Polyglot has a fast and easy to use language classifier that attempts to detect sentence language at parse time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def score_sentences(speeches):\n",
    "    lang_scores = defaultdict(int)\n",
    "    for speech in speeches:\n",
    "        sentences = Poly(\" \".join(speech['text'])).sentences\n",
    "        lang_scores[\"sent_count\"] += len(sentences)\n",
    "        for sent in sentences:\n",
    "                #print(sent)\n",
    "                score, lang = detect_language(sent)\n",
    "                #print(score, lang)\n",
    "                lang_scores[lang] += 1\n",
    "                #print(lang, sent)\n",
    "    return lang_scores\n",
    "\n",
    "def polyglot_score_sentences(speeches):\n",
    "    lang_scores = defaultdict(int)\n",
    "    for speech in speeches:\n",
    "        try:\n",
    "            sentences = Poly(\" \".join(speech['text'])).sentences\n",
    "        except:\n",
    "            print (speech)\n",
    "            break\n",
    "        lang_scores[\"sent_count\"] += len(sentences)\n",
    "        for sent in sentences:\n",
    "            \n",
    "            lang_scores[sent.language.name] += 1\n",
    "            \n",
    "            #if sent.language.confidence < 60.0:\n",
    "                \n",
    "            #my_scorer = detect_language(sent)\n",
    "            #if sent.language.code != my_scorer[1]:\n",
    "            #    print(sent.string)\n",
    "            #    print(sent.language.confidence)\n",
    "            #    print(sent.language.code, my_scorer[1])\n",
    "\n",
    "            #    print(\"-----\")\n",
    "    return lang_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had to do a bit of pre-processing of the texts to clear out a HTML encoding that was breaking the Polyglot parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for d in db.dail.find({}, {'text':True}):\n",
    "    if \"\\x97\" in \"\".join(d['text']):\n",
    "        text = [t.replace(\"\\x97\", \" \") for t in d['text']]\n",
    "                   \n",
    "        db.dail.update_one({\"_id\":d['_id']}, {\"$set\": {\"text\":text}})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_speeches():\n",
    "    return db.dail.find({'date': {\"$gt\": datetime(1982,1,1), \n",
    "                                 \"$lt\": datetime(2002,1,1)}, \n",
    "                        \"len_doc\": {\"$gt\": 100},\n",
    "                        #\"spkr\": \"member/Eamon-de-Valera.D.1919-01-21\"\n",
    "                        },\n",
    "                        {\"text\": True})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of speeches: 175172\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of speeches {:,.0f}:\".format(get_speeches().count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparisons\n",
    "\n",
    "Polyglot is significantly faster than my parser but it classified 50% fewer sentences as Irish. The sentences categorised in French and German are, of course, Francois Mitterand and Helmut Kohl, and they're probably classed as Irish by my parser. On the other hand, that's a difference of 1% Irish according to Polyglot compared to 2% based on my parser. That's probably not significant enough to worry about for the text analysis pipeline, where time is more important than accuracy, but it will need more investigation if I am going to tag sentences for a debates search tool. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 43s, sys: 132 ms, total: 1min 43s\n",
      "Wall time: 1min 45s\n",
      "defaultdict(<class 'int'>, {'French': 114, 'German': 144, 'Irish': 27198, 'English': 3596960, 'sent_count': 3624416})\n",
      "CPU times: user 3min 17s, sys: 168 ms, total: 3min 17s\n",
      "Wall time: 3min 19s\n",
      "defaultdict(<class 'int'>, {'English': 3558957, 'Irish': 65459, 'sent_count': 3624416})\n"
     ]
    }
   ],
   "source": [
    "speeches = get_speeches()\n",
    "\n",
    "%time poly_scores = polyglot_score_sentences(speeches)\n",
    "print(poly_scores)\n",
    "speeches = get_speeches()\n",
    "%time scores = score_sentences(speeches)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.018198423112850852"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "65459/3596960"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4766"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.dail.count({\"spkr\":\"member/Aengus-Ó-Snodaigh.D.2002-06-06\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent = Poly(\"this is a test sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<polyglot.detect.base.Language at 0x7fd27815ac18>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent.language.from_code(\"gd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "members = [{\n",
    "    \"eId\": \"member/Aengus-Ó-Snodaigh.D.2002-06-06\", \n",
    " \"lang\": {'Danish': 1, \n",
    "          'crs': 1, \n",
    "          'Irish': 4506, \n",
    "          'Tatar': 1, \n",
    "          'un': 1, \n",
    "          'Wolof': 1, \n",
    "          'Scots': 4, \n",
    "          'Western Frisian': 3, \n",
    "          'Welsh': 2, \n",
    "          'Scottish Gaelic': 4, \n",
    "          'German': 1, \n",
    "          'sent_count': 47202, \n",
    "          'Manx': 2, \n",
    "          'Hawaiian': 1, \n",
    "          'Samoan': 1, \n",
    "          'English': 42673}\n",
    "    },\n",
    "    {\n",
    "    \"eId\": \"member/Mattie-McGrath.D.2007-06-14\",\n",
    "    \"name\": \"Mattie McGrath\",\n",
    "      \"lang\": {'Lithuanian': 2, 'Western Frisian': 8, 'Esperanto': 1, 'crs': 3, 'Spanish': 1, 'Uzbek': 1, 'Czech': 1, 'Welsh': 2, 'Dutch': 1, 'Hausa': 1, 'Irish': 214, 'English': 39911, 'Luxembourgish': 3, 'Danish': 6, 'Southern Sotho': 1, 'Slovenian': 3, 'Norwegian Nynorsk': 1, 'Xhosa': 3, 'Interlingue': 1, 'un': 3, 'Lingala': 2, 'Estonian': 1, 'Maltese': 1, 'Scottish Gaelic': 11, 'sent_count': 40201, 'Portuguese': 3, 'Manx': 4, 'Scots': 11, 'zzp': 1}  \n",
    "    }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
